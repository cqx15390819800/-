1、
过拟合、欠拟合及其解决方案章节中学会了如何判断过拟合和欠拟合情况
其中涉及K折交叉验证，这在数据集缺少的时候使用
随着训练次数的增加，模型的训练误差肯定是不断减少的，但是泛化误差是先减小后增大
模型的复杂度越大，越易过拟合，数据少也易过拟合。两者是相辅相成的，根据数据集适当选择合适大小的模型
通过曲线判断：
            若为欠拟合，则训练误差比测试误差大，或者差不多
            若为过拟合，则训练误差过小，而测试误差平稳不减
通过正则化的方法可以削减过拟合的情况，其中L1正则比L2正则效果大，而且可以用来挑选特征
在神经网络中使用droupout来解决过拟合现象，即随机丢弃法

2、
梯度消失，梯度爆炸章节中学习到两者的概念
RNN和LSTM当预见过长的序列时，前面的信息难以传到后面，这就是梯度消失，所以transformer模型很好的解决了这个问题
梯度爆炸是由于训练过程中选择了不合适的学习率造成的，这时候就需要根据曲线来调整参数
在房价预测的实战中学会了数据处理和K折交叉验证等前面讲过的方法

3、
循环神经网络进阶章节中学会了GRU、LSTM模型、深层循环神经网络和双向循环神经网络的两种构建方法
bidirectional=True为双向，否则为单向
LSTM：
长短期记忆long short-term memory :
遗忘门:控制上一时间步的记忆细胞 输入门:控制当前时间步的输入
输出门:控制从记忆细胞到隐藏状态
记忆细胞：⼀种特殊的隐藏状态的信息的流动
GRU:
• 重置⻔有助于捕捉时间序列⾥短期的依赖关系；
• 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。
RNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）
⻔控循环神经⽹络：捕捉时间序列中时间步距离较⼤的依赖关系

4、
机器翻译及相关技术章节：
机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。 
主要特征：输出是单词序列而不是单个单词。 
输出序列的长度可能与源序列的长度不同。
其中介绍了Seq2Seq模型，即端到端的模型
应用：应用在对话系统、生成式任务中。
Beam Search使用贪心和维特比算法，其中维特比是DP算法思想

5、
注意力机制与Seq2seq模型章节：
Attention is all your need这篇论文是经典的论文，无论是图像识别，transformer，Seq2Seq都使用了attention结构，用来告诉模型哪一块需要注意
这种想法很聪明，在transformer中使用的是self-attention（encoder和decoder中都有，但是运算机制不同）
该章节中还讲到了点积注意力和softmax屏蔽

6、
Transformer：
分为encoder和decoder层，层中都有attention结构
Add and Norm
除了上面两个模块之外，Transformer还有一个重要的相加归一化层，它可以平滑地整合输入和其他层的输出，
因此我们在每个多头注意力层和FFN层后面都添加一个含残差连接的Layer Norm层。
这里 Layer Norm 与7.5小节的Batch Norm很相似，唯一的区别在于Batch Norm是对于batch size这个维度进行计算均值和方差的，
而Layer Norm则是对最后一维进行计算。层归一化可以防止层内的数值变化过大，从而有利于加快训练速度并且提高泛化性能
其中还讲解了编码器和位置编码
BERT模型就是在Transformer基础上改进的

7、
卷积神经网络基础：
讲解了具体实现的细节，比如shape中的通道等等需要相互对应
讲解了卷积层，padding，pooling等层的简洁实现

8、
LeNet章节：
卷积神经网络就是含卷积层的网络。 LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。
LeNet是比较经典古老的模型，现在出现了很多优秀的模型，比如senet等等，效果更佳

9、
卷积神经网络进阶：
讲解了AlexNet、VGG、NiN、GoogLeNet这些经典的模型结构和代码实现
可以使用迁移学习来调用预训练好的这些网络在小数据中跑结果，经过微调效果还是很不错的！

总结：
通过学习增加了代码实战能力，了解了pytorch的优势，比如构造rnn时，pytorch可以动态搭建网络，这是便捷之处，
框架越来越好，代码量越来越少，便于学习对新手友好而且说明代码不在多，在于价值！
