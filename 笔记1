1、
从线性模型一节中学习到如何手动实现和快速搭建两种方式：
从中学习到了许多函数，例如optim，TensorDatasets等等
以及两种实现方式的比较
从零开始的实现（推荐用来学习）
能够更好的理解模型和神经网络底层的原理
使用pytorch的简洁实现
能够更加快速地完成模型的设计与实现

2、
从softmax一节中学习到如何手动实现softmax和nn.functional里直接调用API两种方式
softmax主要用于神经网络多分类最后一层的处理，是数值归一化的一种方式
课后使用了mnist数据集和resnet进行了实验，效果良好

3、
多层感知机章节中学会了Relu函数，这是常用的激活函数
其中还介绍了许多其他的激活函数，以及相关的数学性质，从中也可以看出缺点和优点，比如sigmoid利于缩放但是容易带来梯度消失
其中在训练过程中快速搭建分类模型，使用nn.Sequential可以实现快速搭建

4、
文本预处理章节学会如何使用open等函数读取文本数据，并对文本数据进行处理
除此之外还介绍了开源的工具进行分词处理（spaCY和NLTK）
自己实现的缺点：
标点符号通常可以提供语义信息，但是我们的方法直接将其丢弃了
类似“shouldn't", "doesn't"这样的词会被错误地处理
类似"Mr.", "Dr."这样的词会被错误地处理

5、
语言模型章节中学会了语言模型的运算机制，通过马尔可夫假设，n-gram运用在模型中，这是一种统计的思路，与后面讲到的rnn和LSTM不同
缺点是稀疏性问题，模型效果并不好
学会了两种采样方式：随机采样和相邻采样

6、
循环神经网络基础章节中学会了手动实现和快速搭建两种方式，通过手动实现深刻了解了运算的机制
其中还涉及到了one-hot编码，可以直接调用API，F.one_hot
由于该网络易造成梯度爆炸，所以介绍了如何裁剪梯度
其中还介绍了困惑度，即越低模型效果越好





